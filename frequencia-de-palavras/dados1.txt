
Tabela peri√≥dica de Mendeleev (1869) [1]
H√° quase 4 anos, em meu primeiro artigo aqui nesse portal ‚ÄúM√°quinas que compreendem a linguagem humana‚Äù, escrevi sobre o estrelado sistema Watson/IBM e seu grande feito no mundo das disputas de conhecimento televisas no melhor estilo ‚ÄúQuem que ser um milion√°rio?‚Äù l√° no in√≠cio dessa d√©cada; n√£o apenas isso, mas principalmente sobre sua enorme capacidade de analisar a linguagem escrita humana a ponto de, por exemplo, processar milhares de artigos cient√≠ficos sobre uma dada prote√≠na associada a muitos tipos de c√¢ncer e identificar sozinho v√°rias outras prote√≠nas relacionadas a esta e que haviam passado desapercebidas pelos pesquisadores e m√©dicos.

Pois bem, essa invej√°vel capacidade cognitiva do Watson se disseminou e evoluiu. Isso √© o que mostra um artigo publicado no in√≠cio deste m√™s na revista Nature [2]. Pesquisadores do Departamento de Energia da Universidade de Berkeley (Calif√≥rnia, EUA) criaram uma m√°quina inteligente capaz de realizar a ‚Äúleitura‚Äù de mais de 3 milh√µes de resumos (abstracts) de artigos cient√≠ficos publicados entre os anos de 1922 e 2018; ou seja, quase 100 anos de pesquisa cient√≠fica foram analisados pela m√°quina. Al√©m de descobrir materiais (elementos qu√≠micos) com propriedades especiais, ela provou seu alto poder de cogni√ß√£o ao recomendar materiais para aplica√ß√µes funcionais v√°rios anos antes de quando essa descoberta foi de fato feita pelos humanos. A t√≠tulo de exemplo, atrav√©s da leitura de trabalhos publicados at√© o ano de 2009, houveram duas descobertas feitas pela m√°quina que os humanos s√≥ chegaram em 2018.

Isso sugere que o conhecimento latente sobre descobertas futuras est√°, em grande parte, embutido em publica√ß√µes j√° existentes, mas permanece desconhecido, simplesmente porque ningu√©m ainda vasculhou o suficiente. O fato √© que particularmente para a pesquisa em Ci√™ncia dos Materiais, a principal fonte de dados interpret√°veis‚Äã‚Äãpor computadores adv√©m de bancos de dados estruturados, dados bem-comportados. O problema √© que esse tipo de dado representa uma fra√ß√£o muito pequena de todo o conhecimento presente na literatura cient√≠fica relacionada e s√≥ acessada atrav√©s da leitura de textos grandes, dif√≠ceis e que demandam muito tempo. O que uma m√°quina inteligente moderna como essa √© capaz de fazer, assim como fora o Watson anteriormente, √© extrair propriedades relevantes e descobrir conex√µes e relacionamentos complexos entre elementos de dados presentes no corpo massivo da literatura cient√≠fica de maneira coletiva.

Bom, a dita cuja foi treinada a partir de um grande conjunto de resumos cient√≠ficos. Esses resumos foram obtidos a partir de bases cient√≠ficas relevantes, tais como a Scopus da Elsevier e a Springer Nature. O aspecto central do modelo criado refere-se √† representa√ß√£o dos elementos de texto, as palavras. Cada palavra √© representada por um vetor multidimensional que preserva seus relacionamentos sint√°ticos e sem√¢nticos com outras palavras, conseguidos atrav√©s de informa√ß√£o sobre co-ocorr√™ncia de palavras nos textos. Na literatura de Processamento de Linguagem Natural (PLN), esse vetor de representa√ß√£o √© chamado de word embeddings [3], que s√£o gerados a partir de algoritmos de aprendizado profundo que dispensam qualquer interven√ß√£o humana; no caso do dom√≠nio desse trabalho, n√£o houve qualquer inser√ß√£o expl√≠cita de conhecimento em Qu√≠mica e, ainda assim, esses embeddings conseguiram capturar conceitos complexos de Ci√™ncia dos Materiais como, por exemplo, a estrutura subjacente da tabela peri√≥dica e as rela√ß√µes estrutura-propriedade nos materiais. O modelo produziu, por exemplo, uma lista ordenada de materiais que eram fortes candidatos a possuir propriedades termoel√©tricas (capacidade de converter calor em energia).

Dentre os algoritmos de aprendizado existentes atualmente para tal fim, os autores utilizaram um dos mais famosos e bem-sucedidos, o chamado Word2vec, particularmente a varia√ß√£o Skip-gram [4]. A hip√≥tese central que norteia a t√©cnica, j√° validada em diferentes trabalhos de PLN, √© a de que j√° que palavras diferentes com significados similares frequentemente aparecem em contextos similares, seus embeddings correspondentes tamb√©m ser√£o similares. O modelo manipulou os vetores para descobrir termos, conceitos e princ√≠pios fundamentais da Ci√™ncia dos Materiais. Dois exemplos ilustram essa capacidade de manipula√ß√£o. Muitas palavras encontradas nos resumos representam composi√ß√µes qu√≠micas de materiais e os cinco materiais mais similares ao LiCoO2 (um composto de c√°todo de √≠on-l√≠tio bem conhecido) pode ser determinado atrav√©s de um produto vetorial (proje√ß√£o) dos embeddings normalizados. De acordo com o modelo treinado, as composi√ß√µes com a maior similaridade ao LiCoO2 foram LiMn2O4, LiNi0.5Mn1.5O4, LiNi0.8Co0.2O2, LiNi0.8Co0.15Al0.05O2 e LiNiO2 ‚Äî todas elas tamb√©m s√£o, n√£o por acaso, materiais de c√°todo de √≠on-l√≠tio. Outro exemplo, diz respeito ao suporte a analogias: ‚ÄòNiFe‚Äô est√° para ‚Äòferromagnetic‚Äô tal como ‚ÄòIrMn‚Äô est√° para ‚Äò?‚Äô. O modelo resolve o problema atrav√©s de uma opera√ß√£o aritm√©tica no espa√ßo vetorial: ferromagnetic ‚Äì NiFe + IrMn ‚âà ‚Ä¶. antiferromagnetic!!! Voil√†! üôÇ

A mensagem passada pelo Watson e seus descendentes √© clara: escolha um campo da Ci√™ncia e eu o ajudarei com insights que voc√™ provavelmente s√≥ ter√° daqui a uma d√©cada, o ajudarei com a cura que demoraria a ser descoberta, com o dinheiro que n√£o seria economizado, com o ve√≠culo que jamais o transportaria, com a estrela que s√≥ seria notada quando n√£o mais existisse, com a tecnologia que s√≥ ‚Äúcompraria‚Äù sua seguran√ßa, sua sa√∫de, sua paz, daqui a muitos e muitos anos. Se o conhecimento existe desde ontem, ‚Äúhoje‚Äù j√° √© muito longe!
